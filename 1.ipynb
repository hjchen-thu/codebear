{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING - ignoring unknown parameter in quantize_config.json: quant_method.\n",
      "2024-03-03 19:41:17 WARNING [src.model.LlamaGPTQ] ignoring unknown parameter in quantize_config.json: quant_method.\n",
      "INFO - The layer lm_head is not quantized.\n",
      "2024-03-03 19:41:17 INFO [src.model.LlamaGPTQ] The layer lm_head is not quantized.\n",
      "2024-03-03 19:41:17 INFO [numexpr.utils] Note: detected 96 virtual cores but NumExpr set to maximum of 64, check \"NUMEXPR_MAX_THREADS\" environment variable.\n",
      "2024-03-03 19:41:17 INFO [numexpr.utils] Note: NumExpr detected 96 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n",
      "2024-03-03 19:41:17 INFO [numexpr.utils] NumExpr defaulting to 8 threads.\n",
      "2024-03-03 19:41:18 WARNING [auto_gptq.nn_modules.qlinear.qlinear_cuda] CUDA extension not installed.\n",
      "2024-03-03 19:41:18 WARNING [auto_gptq.nn_modules.qlinear.qlinear_cuda_old] CUDA extension not installed.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, TextGenerationPipeline\n",
    "from src import LlamaGPTQ\n",
    "\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s %(levelname)s [%(name)s] %(message)s\", level=logging.INFO, datefmt=\"%Y-%m-%d %H:%M:%S\"\n",
    ")\n",
    "\n",
    "pretrained_model_dir = \"/home/chenhj/CodeLlama-7b-Python-hf\"\n",
    "quantized_model_dir = \"/home/chenhj/CodeLlama-7b-4bit\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_dir, use_fast=True)\n",
    "examples = [\n",
    "    tokenizer(\n",
    "        \"auto-gptq is an easy-to-use model quantization library with user-friendly apis, based on GPTQ algorithm.\"\n",
    "    )\n",
    "]\n",
    "\n",
    "# quantize_config = BaseQuantizeConfig(\n",
    "#     bits=4,  # quantize model to 4-bit\n",
    "#     group_size=128,  # it is recommended to set the value to 128\n",
    "#     desc_act=False,  # set to False can significantly speed up inference but the perplexity may slightly bad\n",
    "# )\n",
    "\n",
    "# # load un-quantized model, by default, the model will always be loaded into CPU memory\n",
    "# model = AutoGPTQForCausalLM.from_pretrained(pretrained_model_dir, quantize_config)\n",
    "\n",
    "# # quantize model, the examples should be list of dict whose keys can only be \"input_ids\" and \"attention_mask\"\n",
    "# model.quantize(examples)\n",
    "\n",
    "# # save quantized model\n",
    "# model.save_quantized(quantized_model_dir)\n",
    "\n",
    "# # save quantized model using safetensors\n",
    "# model.save_quantized(quantized_model_dir, use_safetensors=True)\n",
    "\n",
    "# push quantized model to Hugging Face Hub.\n",
    "# to use use_auth_token=True, Login first via huggingface-cli login.\n",
    "# or pass explcit token with: use_auth_token=\"hf_xxxxxxx\"\n",
    "# (uncomment the following three lines to enable this feature)\n",
    "# repo_id = f\"YourUserName/{quantized_model_dir}\"\n",
    "# commit_message = f\"AutoGPTQ model for {pretrained_model_dir}: {quantize_config.bits}bits, gr{quantize_config.group_size}, desc_act={quantize_config.desc_act}\"\n",
    "# model.push_to_hub(repo_id, commit_message=commit_message, use_auth_token=True)\n",
    "\n",
    "# alternatively you can save and push at the same time\n",
    "# (uncomment the following three lines to enable this feature)\n",
    "# repo_id = f\"YourUserName/{quantized_model_dir}\"\n",
    "# commit_message = f\"AutoGPTQ model for {pretrained_model_dir}: {quantize_config.bits}bits, gr{quantize_config.group_size}, desc_act={quantize_config.desc_act}\"\n",
    "# model.push_to_hub(repo_id, save_dir=quantized_model_dir, use_safetensors=True, commit_message=commit_message, use_auth_token=True)\n",
    "\n",
    "# load quantized model to the first GPU\n",
    "model = LlamaGPTQ.from_quantized(quantized_model_dir, device=\"cuda:0\")\n",
    "\n",
    "# download quantized model from Hugging Face Hub and load to the first GPU\n",
    "# model = AutoGPTQForCausalLM.from_quantized(repo_id, device=\"cuda:0\", use_safetensors=True, use_triton=False)\n",
    "\n",
    "# inference with model.generate\n",
    "# print(tokenizer.decode(model.generate(**tokenizer(\"auto_gptq is\", return_tensors=\"pt\").to(model.device))[0]))\n",
    "\n",
    "# or you can also use pipeline\n",
    "# pipeline = TextGenerationPipeline(model=model, tokenizer=tokenizer)\n",
    "# print(pipeline(\"auto-gptq is\")[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "/home/chenhj/anaconda3/lib/python3.11/site-packages/transformers/generation/utils.py:1133: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> auto_gptq is a tool to automatically generate a GPTQ file from a G\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(model.generate(**tokenizer(\"auto_gptq is\", return_tensors=\"pt\").to(model.device))[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
