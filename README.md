# Codebear
This repository combines **gptq 4-bit quantization** and **speculative decoding** to accelerate Large Language Models' (LLM) inference in **personal usage scenarios**  (where GPU resources are limited yet there's a pursuit for better performance with larger models).


<!-- # Update  -->

## Quick Tour

## Future Plans

## References

## Acknowledgement